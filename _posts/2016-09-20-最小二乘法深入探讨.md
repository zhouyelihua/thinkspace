---
layout: post
title: "最小二乘法深入探讨"
description: "探讨最小二乘法，并变成实现相关的实现"
comments: false
keywords: "最小二乘"
---

&emsp;在机器学习当中经常涉及到对能量方程，或者是目标函数进行优化的过程。可以用公式表示为：
<div style="text-align:center">
$$min \qquad f(x)$$

$$s.t.\qquad  c_i(x)=0,   i=1,...,m$$

$$     \qquad\qquad c_i(x)>=0,   i=m,...,n$$
</div>
&emsp;因为所有的优化问题都可以分为有约束的和无约束的，在无约束的优化问题中上面的公式有$$n=0$$
.同样的优化问题都可以转化为求最小值问题，求max的问题取其相反数即可。

&emsp;最小二乘问题优化一般是通过模型的函数$$f_\theta(x_i)$$与输出$$y_i$$的误差

&emsp;其中最小二乘的意思就是对误差进行取平方所以最小二乘法的数学模型就是:

$$min\qquad \sum_{i=1}^n ( f_\theta(x_i)-y_i)^2$$

&emsp;最小二乘法就是通过计算得到$$f_\theta(x_i)$$的一个参数模型$$\theta$$使得上述的误差达到最小。

&emsp;当我们采用的是线性模型的时候，有：

$$f_\theta(x_i)=\sum_{j=1}^b\theta_i\phi_i(x)=\theta^T\phi(x_i)$$

&emsp;这个是通过把函数转化为向量乘积的形式便于计算，公式中基函数$$\phi(x)$$一般可以分为两种情况：

&emsp;1.幂函数

$$\mathbf{\phi(x)}=(1,x,x^2,x^3,...)$$

&emsp;幂函数拟合的光滑性比较好，局部效果较好

&emsp;2.正弦（余弦）函数

$$\mathbf{\phi(x)}=(1,\sin\frac{x}{2},\cos\frac{x}{2},\sin\frac{2x}{2},\cos\frac{2x}{2},...)$$

&emsp;光滑性较差，但是全局性较好（特别是针对特殊问题）

&emsp;那么上面提到的优化问题就转化为

$$J(\mathbf{\theta})=\frac{1}{2}\|\mathbf{\phi}\mathbf{\theta}-\mathbf{y}\|^2$$

那么问题就转化为矩阵的问题(注意上文中$$\phi(x_i)$$不同于$$\phi$$)

其中

![公式](https://bytebucket.org/zhouyelihua/markdownphoto/raw/4d0da26257ebc00483d615e2210f20dd62668d29/latx.png)
&emsp;&emsp;$$\mathbf{y}=[y_1,y_2,...,y_m]^T$$这个是我们的监督学习训练参数输出的n维向量，那么我们对上面的矩阵进行扩展可以得到以下公式

$$\mathbf{\phi}\mathbf{\theta}= 
\begin{vmatrix}
\phi(x_1)&\\
\vdots&\\
\phi(x_n)&\\
\end{vmatrix}
\mathbf{\theta}=
\begin{vmatrix}
\phi(x_{11})&\phi(x_{12})&\cdots&\phi(x_{1n})\\
\vdots&\\
\phi(x_{m1})&\phi(x_{m2})&\cdots&\phi(x_{mn})\\
\end{vmatrix}
\begin{vmatrix}
\phi(\theta_1)&\\
\vdots&\\
\phi(\theta_n)&\\
\end{vmatrix}
$$

在公式：
&emsp;&emsp;$$J(\mathbf{\theta})=\frac{1}{2}\|\mathbf{\phi}\mathbf{\theta}-\mathbf{y}\|^2$$
我们可以对该式子进行微分可以得到：










